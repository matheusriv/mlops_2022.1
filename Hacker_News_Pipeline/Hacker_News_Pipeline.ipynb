{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided project: hacker news pipeline\n",
    "\n",
    "In this guided project, we will use the pipeline we have been building in the 'Building a Data Pipeline' course of Dataquest, and apply it to a real world data pipeline project. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "To make things easier, we have already downloaded a list of JSON posts to a file called hn_stories_2014.json. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "* url: The URL of the story link.\n",
    "* objectID: The ID of the story.\n",
    "* author: The story's author (username on HN).\n",
    "* points: The number of upvotes the story had.\n",
    "* title: The headline of the post.\n",
    "* num_comments: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "from datetime import datetime\n",
    "from stop_words import stop_words\n",
    "import json\n",
    "import csv\n",
    "import io\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "# we need to filter out most popular stories every year\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return (story['points'] > 50 and\n",
    "                story['num_comments'] > 1 and not\n",
    "                story['title'].startswith('Ask HN'))\n",
    "    \n",
    "    return (\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )\n",
    "\n",
    "# Now dict to csv conversion is necessary.\n",
    "# The purpose of translating the dictionaries to a CSV is that \n",
    "# we want to have a consistent data format when running the later \n",
    "# summarizations. By keeping consistent data formats, \n",
    "# each of your pipeline tasks will be adaptable with future task \n",
    "# requirements.\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'],\n",
    "             datetime.strptime(story['created_at'],\n",
    "                               \"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "             story['url'], story['points'], story['title'])\n",
    "        )\n",
    "        \n",
    "    return build_csv(lines, header=['objectID', 'created_at',\n",
    "                                    'url', 'points', 'title'],\n",
    "                     file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    return (line[idx] for line in reader)\n",
    "\n",
    "# To clean the titles, we should make sure to lower case \n",
    "# the titles, and to remove the punctuation.\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title\n",
    "\n",
    "# With a cleaned title, we can now build the word frequency dictionary. \n",
    "# A word frequency dictionary are key value pairs that connects a word to \n",
    "# the number of times it is used in a text.\n",
    "# To find actual keywords, we should enforce the word frequency dictionary \n",
    "# to not include stop words. We have included a module called stop_words \n",
    "# with a tuple of the most common used stop words in the English language.\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "# return a list of the top 100 tuples\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_words(keyword_dictionary):\n",
    "    top_values = sorted(keyword_dictionary.items(),\n",
    "                        key=lambda x:x[1], reverse=True)\n",
    "    return top_values[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "# Run the pipline using pipeline.run(), and print the ouput of the new task function.\n",
    "ran = pipeline.run()\n",
    "print(ran[top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
